{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and clean data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train = df_train.drop(columns=[\"zipcode\"])\n",
    "\n",
    "columns = df_train.drop(columns=[\"price\"]).columns.tolist()\n",
    "df_train[columns] = (df_train[columns] - df_train[columns].mean()) / df_train[columns].std()\n",
    "df_train[\"price\"] = df_train[\"price\"] / 1000\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test = df_test.drop(columns=[\"zipcode\", \"id\", \"date\"])\n",
    "\n",
    "columns = df_test.drop(columns=[\"price\"]).columns.tolist()\n",
    "df_test[columns] = (df_test[columns] - df_test[columns].mean()) / df_test[columns].std()\n",
    "df_test[\"price\"] = df_test[\"price\"] / 1000\n",
    "\n",
    "#print(df_test.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set training and testing data\n",
    "\n",
    "X_train = df_train.drop(columns=[\"price\"])\n",
    "y_train = df_train[\"price\"]\n",
    "X_test = df_test.drop(columns=[\"price\"])\n",
    "y_test = df_test[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define Lienar Regression with Gradient Descent\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    \n",
    "    def __init__(self, alpha=0.01, iterations=100):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        self.theta = np.zeros(X_b.shape[1])\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            predictions = X_b @ self.theta\n",
    "            errors = predictions - y\n",
    "            \n",
    "            gradient = (2/n) * (errors.T @ X_b)\n",
    "            self.theta -= self.alpha * gradient\n",
    "            \n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        return X_b @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a  iterations      Train MSE      Train R^2       Test MSE       Test R^2\n",
      "0  0.01          10   2.357554e+05  -1.048000e+00   2.931676e+05  -7.580000e-01\n",
      "1  0.01          50   6.970036e+04   3.950000e-01   1.079586e+05   3.520000e-01\n",
      "2  0.01         100   3.676615e+04   6.810000e-01   6.896546e+04   5.860000e-01\n",
      "3  0.10          10   3.504898e+04   6.960000e-01   6.661907e+04   6.000000e-01\n",
      "4  0.10          50   3.142711e+04   7.270000e-01   6.000553e+04   6.400000e-01\n",
      "5  0.10         100   3.141602e+04   7.270000e-01   5.990571e+04   6.410000e-01\n",
      "6  0.50          10   1.428612e+17  -1.240791e+12   1.395471e+17  -8.369797e+11\n",
      "7  0.50          50   1.143174e+67  -9.928799e+61   1.116654e+67  -6.697502e+61\n",
      "8  0.50         100  2.735960e+129 -2.376262e+124  2.672491e+129 -1.602915e+124\n"
     ]
    }
   ],
   "source": [
    "## Train model with different alphas and iterations\n",
    "\n",
    "alphas = [0.01, 0.1, 0.5]\n",
    "iterations = [10, 50, 100]\n",
    "results = [('a', 'iterations', 'Train MSE', 'Train R^2', 'Test MSE', 'Test R^2')]\n",
    "\n",
    "for a in alphas:\n",
    "    for i in iterations:\n",
    "        model = LinearRegressionGD(a, i)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "        results.append((a, i, train_mse, train_r2, test_mse, test_r2))\n",
    "\n",
    "results_df = pd.DataFrame(results[1:], columns=results[0])\n",
    "\n",
    "print(results_df.round(3).to_string())     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at alpha 0.01, we can see that the at 10 iterations the R^2 is terrible for both training and testing, showing that our model is doing a very bad job at predictions and that 10 iterations is not enough. At 100, the R^2 is 0.68 and 0.58 for training and test data respectively, meaning that through 100 iterations the model does improve a significant amount, but lots of iterations are needed for such a small alpha. With alpha 0.1, we can see that between 50 iterations and 100 iterations, the MSE and R^2 for both training and testing is about the same, meaning that the model has converged to an optimal solution. This also means that this solution is reached at at least 50 iterations, meaning that much less iterations are needed for this value of alpha. When observing alpha 0.5, we can see that the MSE and R^2 values reach astronomical values, showing that this step size is much too large and ends up diverging rather than converging. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
