{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import linear regression model from q5\n",
    "class LinearRegressionGD:\n",
    "    \n",
    "    def __init__(self, alpha=0.01, iterations=100):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        self.theta = np.zeros(X_b.shape[1])\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            predictions = X_b @ self.theta\n",
    "            errors = predictions - y\n",
    "            \n",
    "            gradient = (2/n) * (errors.T @ X_b)\n",
    "            self.theta -= self.alpha * gradient\n",
    "            \n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        return X_b @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define ridge regression using gradient descent\n",
    "class RidgeRegressionGD:\n",
    "    \n",
    "    def __init__(self, alpha=0.01, iterations=100, lam=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.lam = lam\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        self.theta = np.zeros(X_b.shape[1])\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            predictions = X_b @ self.theta\n",
    "            errors = predictions - y\n",
    "\n",
    "            reg_term = self.lam * self.theta.copy()\n",
    "            reg_term[0] = 0\n",
    "            \n",
    "            gradient = (2 / n) * (errors.T @ errors + reg_term)\n",
    "            self.theta -= self.alpha * gradient\n",
    "            \n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        ones = np.ones((n, 1))\n",
    "        X_b = np.hstack((ones, X))\n",
    "        return X_b @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initalize data\n",
    "np.random.seed(42)\n",
    "N = 1000\n",
    "\n",
    "X = np.random.uniform(-2, 2, (N, 1))\n",
    "e = np.random.normal(0, 2, N)\n",
    "y = 1 + 2 * X.flatten() + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a  iterations  MSE    R^2\n",
      "0  0.1          50  3.9  0.564\n",
      "1  0.1         100  3.9  0.564\n"
     ]
    }
   ],
   "source": [
    "## Train using linear regression for comparison\n",
    "alphas = [0.1]\n",
    "iterations = [50, 100]\n",
    "results = [('a', 'iterations', 'MSE', 'R^2')]\n",
    "\n",
    "for a in alphas:\n",
    "    for i in iterations:\n",
    "        model = LinearRegressionGD(a, i)\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "        results.append((a, i, mse, r2))\n",
    "\n",
    "results_df = pd.DataFrame(results[1:], columns=results[0])\n",
    "\n",
    "print(results_df.round(3).to_string()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lambda         slope           MSE           R^2\n",
      "0       1  1.921000e+00  3.900000e+00  5.640000e-01\n",
      "1      10  1.909000e+00  3.900000e+00  5.640000e-01\n",
      "2     100  1.791000e+00  3.923000e+00  5.610000e-01\n",
      "3    1000  1.109000e+00  4.802000e+00  4.630000e-01\n",
      "4   10000 -6.974259e+09  6.644111e+19 -7.429968e+18\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "\n",
    "lam = [1, 10, 100, 1000, 10000]\n",
    "results = [(\"lambda\", 'slope', 'MSE', 'R^2')]\n",
    "\n",
    "for l in lam:\n",
    "    model = RidgeRegressionGD(0.1, 100, l)\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    results.append((l, model.theta[1], mse, r2))\n",
    "\n",
    "results_df = pd.DataFrame(results[1:], columns=results[0])\n",
    "\n",
    "print(results_df.round(3).to_string())     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the linear regression, the results are almost exactly the same, with a MSE of 3.9 and R^2 of 0.564. However, one notable change is that the higher the value of lambda goes, the worse the model gets. It stays about the same for lambda=1 and lambda = 10, but at 100 we see a slight drop off and by 10000 the model has gotten very bad. This means that lambda works better for lower numbers and will get worse at a certain point when it is increased too much. We can also see the slope decrease slightly as lambda increase, by 10000 getting to a very large negative number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
